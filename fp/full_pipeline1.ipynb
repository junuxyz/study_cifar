{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f09fbd",
   "metadata": {},
   "source": [
    "## 01. Configuration For Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218d0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # ========================\n",
    "    # Dataset\n",
    "    # ========================\n",
    "    'dataset': 'CIFAR100',\n",
    "    'num_classes': 100,\n",
    "    'batch_size': 256,\n",
    "    'val_ratio': 0.1,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "\n",
    "    # ========================\n",
    "    # Transfer Learning\n",
    "    # ========================\n",
    "    # use_pretrained=True이면 아래 설정들이 자동으로 결정됨:\n",
    "    #   - mean/std: ImageNet 자동 적용\n",
    "    #   - input_size: 모델에서 자동 결정\n",
    "    #   - strong_aug: True (자동)\n",
    "    #   - augmentation: bicubic resize, auto_augment 포함\n",
    "    'use_pretrained': True,  # True면 Transfer Learning 모드\n",
    "    'pretrained_model_name': 'vit_base_patch16_224',  # timm 모델 이름\n",
    "    \n",
    "    # Supported pretrained models (timm):\n",
    "    #  - 'resnet50', 'resnet101': Classical ResNet\n",
    "    #  - 'convnext_small', 'convnext_base': Modern ConvNet\n",
    "    #  - 'vit_base_patch16_224': Vision Transformer\n",
    "    #  - 'deit_base_patch16_224': Data-efficient ViT\n",
    "    #  - 'vit_large_patch14_224_in21k': Larger ViT\n",
    "\n",
    "    # ========================\n",
    "    # Augmentation (use_pretrained=False일 때만 적용)\n",
    "    # ========================\n",
    "    'strong_aug': False,\n",
    "    'flip': True,\n",
    "    'erase': False,\n",
    "    'color_jitter': False,\n",
    "\n",
    "    # ========================\n",
    "    # Model Architecture\n",
    "    # ========================\n",
    "    'model': 'convnext_s',\n",
    "    'num_classes': 100,\n",
    "    'dropout': 0.0,\n",
    "\n",
    "    # ========================\n",
    "    # Training\n",
    "    # ========================\n",
    "    'epochs': 1,\n",
    "    'lr': 0.001,\n",
    "    'lr_scheduler': 'cosine',  # 'step', 'cosine', 'onecycle', 'exponential'\n",
    "    'lr_decay_factor': 0.9,    # Multiplicative decay when lr_scheduler='exponential'\n",
    "    'weight_decay': 1e-4,\n",
    "\n",
    "    # ========================\n",
    "    # Fine-tuning Strategy\n",
    "    # ========================\n",
    "    # Transfer Learning에서 중요한 설정\n",
    "    'freeze_backbone': True,       # 처음에 backbone freeze할지\n",
    "    'freeze_epochs': 10,           # N epoch 동안 backbone freeze 유지\n",
    "    'unfreeze_strategy': 'gradual', # 'gradual': epoch마다 unfreeze, 'all_at_once': freeze_epochs 후 한번에\n",
    "    \n",
    "    # Gradual unfreezing 관련\n",
    "    'unfreeze_every_n_epochs': 5,  # 5 epoch마다 한 층씩 unfreeze\n",
    "    'layer_lr_decay': 0.1,         # 뒤로 갈수록 learning rate 0.1배씩 감소\n",
    "\n",
    "    # ========================\n",
    "    # Checkpointing\n",
    "    # ========================\n",
    "    'checkpoint_dir': 'checkpoints',\n",
    "    'resume_from_checkpoint': False,\n",
    "    'checkpoint_interval': 10,\n",
    "\n",
    "    # ========================\n",
    "    # Logging\n",
    "    # ========================\n",
    "    'project': 'transfer_learning',\n",
    "    'run_name': None,  # Auto-generated if None\n",
    "    'log_interval': 50,  # Log every N batches\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9b252",
   "metadata": {},
   "source": [
    "## 02. Load and Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0e796c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpotw/Projects/study/study_cifar/.venv/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/jpotw/Projects/study/study_cifar/.venv/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from typing import Optional\n",
    "import timm\n",
    "from timm.data import create_transform\n",
    "\n",
    "DEFAULT_STATS = {\n",
    "    \"CIFAR10\":  ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    \"CIFAR100\": ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "    \"FashionMNIST\": ((0.2860,), (0.3530,)),\n",
    "}\n",
    "\n",
    "def get_class_names(dataset_name, root=\"./data\", train=True):\n",
    "    \"\"\"Return list of class label names for a torchvision dataset\"\"\"\n",
    "    ds_class = getattr(torchvision.datasets, dataset_name)\n",
    "    dataset = ds_class(root=root, train=train, download=True)\n",
    "    if hasattr(dataset, \"classes\"):\n",
    "        return dataset.classes\n",
    "    elif hasattr(dataset, \"labels\"):\n",
    "        return dataset.labels\n",
    "    else:\n",
    "        raise AttributeError(f\"{dataset_name} has no attribute 'classes' or 'labels'\")\n",
    "\n",
    "def compute_mean_std(dataset):\n",
    "    \"\"\"ToTensor()만 적용된 dataset에서 채널별 mean/std 계산\"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=1024, shuffle=False)\n",
    "    n = 0\n",
    "    s1, s2 = 0.0, 0.0\n",
    "    for x, _ in loader:\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, -1)\n",
    "        n += b * h * w\n",
    "        s1 += x.sum(dim=(0, 2))\n",
    "        s2 += (x ** 2).sum(dim=(0, 2))\n",
    "    mean = s1 / n\n",
    "    std = torch.sqrt(s2 / n - mean ** 2)\n",
    "    return tuple(mean.tolist()), tuple(std.tolist())\n",
    "\n",
    "def get_transforms(image_size, mean, std, *, \n",
    "                  strong_aug=True, flip=True,\n",
    "                  erase=False, pretrained_input_size=None, use_pretrained=False):\n",
    "    \"\"\"\n",
    "    Transform 생성.\n",
    "    \n",
    "    Args:\n",
    "        use_pretrained: True면 timm의 최적화된 transform 사용 (ImageNet 기준)\n",
    "        pretrained_input_size: Pretrained 모델의 input size\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pretrained 모델용 최적화된 transform (timm)\n",
    "    if use_pretrained and pretrained_input_size is not None:\n",
    "        transform_train = create_transform(\n",
    "            input_size=pretrained_input_size,\n",
    "            is_training=True,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "            auto_augment='rand-m9-mstd0.5' if strong_aug else None,\n",
    "            interpolation='bicubic',\n",
    "            hflip=flip,\n",
    "            re_prob=0.25 if erase else 0.0,\n",
    "        )\n",
    "        transform_test = create_transform(\n",
    "            input_size=pretrained_input_size,\n",
    "            is_training=False,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "            interpolation='bicubic',\n",
    "        )\n",
    "        return transform_train, transform_test\n",
    "    \n",
    "    # 일반 학습용 transform\n",
    "    train_trans = []\n",
    "    test_trans = []\n",
    "    \n",
    "    if pretrained_input_size is not None:\n",
    "        train_trans.append(T.Resize(pretrained_input_size, interpolation=T.InterpolationMode.BICUBIC))\n",
    "        test_trans.append(T.Resize(pretrained_input_size, interpolation=T.InterpolationMode.BICUBIC))\n",
    "    else:\n",
    "        train_trans.append(T.RandomCrop(image_size, padding=4))\n",
    "    \n",
    "    if flip:\n",
    "        train_trans.append(T.RandomHorizontalFlip())\n",
    "    if strong_aug:\n",
    "        train_trans.insert(0, T.TrivialAugmentWide())\n",
    "    \n",
    "    train_trans += [T.ToTensor(), T.Normalize(mean, std)]\n",
    "    test_trans += [T.ToTensor(), T.Normalize(mean, std)]\n",
    "    \n",
    "    if erase:\n",
    "        train_trans.append(T.RandomErasing(p=0.5))\n",
    "    \n",
    "    return T.Compose(train_trans), T.Compose(test_trans)\n",
    "\n",
    "def get_dataloaders(\n",
    "    dataset_name: str = \"CIFAR10\",\n",
    "    batch_size: int = 128,\n",
    "    data_root: str = \"./data\",\n",
    "    val_ratio: float = 0.1,\n",
    "    num_workers: int = 4,\n",
    "    pin_memory: bool = True,\n",
    "    use_pretrained: bool = False,\n",
    "    pretrained_model_name: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    train_loader, val_loader, test_loader 자동 구성.\n",
    "    \n",
    "    Transfer Learning 사용 시:\n",
    "        use_pretrained=True 로 설정하면 모든 설정이 자동으로 적용됨\n",
    "        - ImageNet 통계 (mean/std)\n",
    "        - timm 최적화 transform (bicubic resize, augmentation)\n",
    "        - 적절한 augmentation (강함)\n",
    "    \n",
    "    Args:\n",
    "        use_pretrained: Pretrained 모델 사용 여부\n",
    "        pretrained_model_name: timm 모델 이름 (예: 'resnet50', 'vit_base_patch16_224')\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================\n",
    "    # Transfer Learning 설정\n",
    "    # ========================\n",
    "    if use_pretrained:\n",
    "        # timm에서 모델 정보 가져오기\n",
    "        try:\n",
    "            model_info = timm.get_model_pretrained_cfg(pretrained_model_name)\n",
    "            pretrained_input_size = model_info.input_size[-1]  # (3, 224, 224) → 224\n",
    "        except:\n",
    "            # 모델 정보를 못 가져온 경우 기본값\n",
    "            pretrained_input_size = 224\n",
    "        \n",
    "        # ImageNet 통계 자동 적용\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        \n",
    "        # Pretrained는 항상 강한 augmentation 적용\n",
    "        strong_aug = True\n",
    "        flip = True\n",
    "        erase = True\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[Transfer Learning Mode]\")\n",
    "        print(f\"Model: {pretrained_model_name}\")\n",
    "        print(f\"Input size: {pretrained_input_size}x{pretrained_input_size}\")\n",
    "        print(f\"Statistics: ImageNet (mean={mean}, std={std})\")\n",
    "        print(f\"Augmentation: Strong (auto_augment, flip, erase)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # ========================\n",
    "    # 일반 학습 설정\n",
    "    # ========================\n",
    "    else:\n",
    "        pretrained_input_size = None\n",
    "        strong_aug = True\n",
    "        flip = True\n",
    "        erase = False\n",
    "        \n",
    "        if dataset_name in DEFAULT_STATS:\n",
    "            mean, std = DEFAULT_STATS[dataset_name]\n",
    "        else:\n",
    "            tmp = getattr(torchvision.datasets, dataset_name)(\n",
    "                root=data_root, train=True, download=True, transform=T.ToTensor()\n",
    "            )\n",
    "            mean, std = compute_mean_std(tmp)\n",
    "        \n",
    "        print(f\"\\n[Standard Training Mode] Dataset: {dataset_name}\")\n",
    "    \n",
    "    # Transform 생성\n",
    "    image_size = pretrained_input_size if pretrained_input_size else 32\n",
    "    transform_train, transform_test = get_transforms(\n",
    "        image_size, mean, std,\n",
    "        strong_aug=strong_aug,\n",
    "        flip=flip,\n",
    "        erase=erase,\n",
    "        pretrained_input_size=pretrained_input_size,\n",
    "        use_pretrained=config['use_pretrained'],\n",
    "    )\n",
    "    \n",
    "    # Dataset 로드\n",
    "    ds_class = getattr(torchvision.datasets, dataset_name)\n",
    "    full_train = ds_class(root=data_root, train=True, download=True, transform=transform_train)\n",
    "    test_set = ds_class(root=data_root, train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    # Validation split\n",
    "    val_size = int(len(full_train) * val_ratio)\n",
    "    train_size = len(full_train) - val_size\n",
    "    train_set, val_set = random_split(full_train, [train_size, val_size])\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=config['batch_size'], shuffle=True,\n",
    "        num_workers=config['num_workers'], pin_memory=config['pin_memory'], persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=config['batch_size'], shuffle=False,\n",
    "        num_workers=config['num_workers'], pin_memory=config['pin_memory'], persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=config['batch_size'], shuffle=False,\n",
    "        num_workers=config['num_workers'], pin_memory=config['pin_memory'], persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Mean/Std: {mean} / {std}\")\n",
    "    print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n",
    "    print(f\"Batch size: {batch_size}, Num workers: {num_workers}, Pin memory: {pin_memory}\\n\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e73ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# WideResNet (from scratch)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout_rate=0.0, stride=1):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_ch)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.shortcut = (\n",
    "            nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)\n",
    "            if (stride != 1 or in_ch != out_ch) else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        x = self.conv1(self.relu1(self.bn1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(self.relu2(self.bn2(x)))\n",
    "        return x + res\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, widen_factor=10, dropout_rate=0.3, num_classes=10):\n",
    "        super().__init__()\n",
    "        assert ((depth - 4) % 6 == 0), \"depth should be 6n+4\"\n",
    "        N = (depth - 4) // 6\n",
    "        K = widen_factor\n",
    "\n",
    "        self.stem = nn.Conv2d(3, 16, 3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_group(16, 16*K, N, dropout_rate, stride=1)\n",
    "        self.layer2 = self._make_group(16*K, 32*K, N, dropout_rate, stride=2)\n",
    "        self.layer3 = self._make_group(32*K, 64*K, N, dropout_rate, stride=2)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm2d(64*K),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*K, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_group(self, in_ch, out_ch, N, dropout_rate, stride):\n",
    "        layers = [Block(in_ch, out_ch, dropout_rate, stride)]\n",
    "        for _ in range(1, N):\n",
    "            layers.append(Block(out_ch, out_ch, dropout_rate, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "def _make_resnet18_cifar(num_classes=10, pretrained=False, dropout=0.0):\n",
    "    if pretrained:\n",
    "        weights = torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "    else:\n",
    "        weights = None\n",
    "    m = torchvision.models.resnet18(weights=weights)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    in_feat = m.fc.in_features\n",
    "    head = [nn.Linear(in_feat, num_classes)]\n",
    "    if dropout > 0:\n",
    "        # Create a list of layers for the head\n",
    "        head = [nn.Dropout(dropout), nn.Linear(in_feat, num_classes)]\n",
    "    # Use nn.Sequential to combine the layers in the head\n",
    "    m.fc = nn.Sequential(*head)\n",
    "    return m\n",
    "\n",
    "\n",
    "def get_model(model_name, num_classes=10, dropout=0.0, pretrained=True):\n",
    "    \"\"\"\n",
    "    Supported models:\n",
    "      - 'wrn28_10': good for cifar-10/100 (only when pixels are 32x32 or 64x64)\n",
    "      - 'convnext_s': small gpu, fast baseline\n",
    "      - 'vit_b16_aug': good accuracy, moderate gpu\n",
    "      - 'deit_b16d': good accuracy, moderate gpu\n",
    "      - 'eva_b14': optional heavy\n",
    "      - 'resnet18_cifar': classic baseline for cifar-10/100\n",
    "    \"\"\"\n",
    "    name = model_name.lower()\n",
    "    timm_map = {\n",
    "        'convnext_s':  'convnext_small.fb_in22k_ft_in1k',\n",
    "        'deit_b16d':   'deit_base_distilled_patch16_224',\n",
    "        'vit_b16_aug': 'vit_base_patch16_224.augreg_in21k_ft_in1k',\n",
    "        'eva_b14':     'eva02_base_patch14_224',\n",
    "    }\n",
    "\n",
    "    if name in timm_map:\n",
    "        # timm은 drop_rate, drop_path_rate 둘 다 지원. 커스텀 데이터셋은 dpr 0.1~0.2 추천.\n",
    "        return timm.create_model(\n",
    "            timm_map[name],\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            drop_rate=dropout,\n",
    "            drop_path_rate=0.1\n",
    "        )\n",
    "    if name == 'wrn28_10':\n",
    "        return WideResNet(depth=28, widen_factor=10, dropout_rate=dropout, num_classes=num_classes)\n",
    "\n",
    "    if name == 'resnet18_cifar':\n",
    "        return _make_resnet18_cifar(num_classes=num_classes, pretrained=pretrained, dropout=dropout)\n",
    "\n",
    "    raise ValueError(f\"Unknown model: {model_name}. Choose from \"\n",
    "                     f\"['wrn28_10','convnext_s','deit_b16d','vit_b16_aug','eva_b14','resnet18_cifar'].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df556eb",
   "metadata": {},
   "source": [
    "### 3-1. Helper Functions for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3102a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# 1) output fc layer label 맞추기\n",
    "def replace_classifier(model, num_classes: int) -> None:\n",
    "    \"\"\"\n",
    "    모델의 최종 분류 헤드를 num_classes에 맞게 교체.\n",
    "    torchvision(fc), timm(classifier/head) 모두 대응.\n",
    "    \"\"\"\n",
    "    # torchvision: resnet 등\n",
    "    if hasattr(model, \"fc\") and isinstance(model.fc, nn.Linear):\n",
    "        in_f = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_f, num_classes)\n",
    "        return\n",
    "    # timm: efficientnet 등\n",
    "    if hasattr(model, \"classifier\") and isinstance(model.classifier, nn.Linear):\n",
    "        in_f = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(in_f, num_classes)\n",
    "        return\n",
    "    # timm: convnext 등\n",
    "    if hasattr(model, \"head\") and isinstance(model.head, nn.Linear):\n",
    "        in_f = model.head.in_features\n",
    "        model.head = nn.Linear(in_f, num_classes)\n",
    "        return\n",
    "    raise RuntimeError(\"No replaceable classifier head found (fc/classifier/head).\")\n",
    "\n",
    "# 2) freeze 정도 (백본만 얼리기 or 해제)\n",
    "def apply_freeze(model, freeze_backbone: bool, head_names=(\"fc\",\"classifier\",\"head\")) -> None:\n",
    "    \"\"\"\n",
    "    freeze_backbone=True면 헤드만 학습, False면 전부 학습.\n",
    "    \"\"\"\n",
    "    for name, p in model.named_parameters():\n",
    "        p.requires_grad = (not freeze_backbone) or any(h in name for h in head_names)\n",
    "\n",
    "# 3) lr (head) 변화: head(학습하는 부분)/backbone(고정하는 부분) 서로 다른 LR용 파라미터 그룹 만들기\n",
    "def make_param_groups(model, lr_head: float, lr_backbone: float, weight_decay: float = 5e-4, momentum: float = 0.9,\n",
    "                      head_names=(\"fc\",\"classifier\",\"head\")):\n",
    "    \"\"\"\n",
    "    optimizer에 바로 넣을 수 있는 param_groups 반환.\n",
    "    SGD/AdamW 등 어떤 옵티마이저에도 그대로 사용 가능.\n",
    "    \"\"\"\n",
    "    head_params, backbone_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        (head_params if any(h in n for h in head_names) else backbone_params).append(p)\n",
    "\n",
    "    pg = []\n",
    "    if backbone_params:\n",
    "        pg.append(dict(params=backbone_params, lr=lr_backbone, weight_decay=config['weight_decay'], momentum=momentum))\n",
    "    if head_params:\n",
    "        pg.append(dict(params=head_params, lr=lr_head, weight_decay=config['weight_decay'], momentum=momentum))\n",
    "    return pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add93641",
   "metadata": {},
   "source": [
    "### 03-2. Save and Load Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d49b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "  \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "  Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "\n",
    "  Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "  target_dir_path = Path(target_dir)\n",
    "  target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "  # Create model save path\n",
    "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "  model_save_path = target_dir_path / model_name\n",
    "\n",
    "  # Save the model state_dict()\n",
    "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "  torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)\n",
    "\n",
    "\n",
    "def save_checkpoint(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    epoch: int,\n",
    "                    target_dir: str,\n",
    "                    checkpoint_name: str,\n",
    "                    scheduler=None,\n",
    "                    best_acc: float = 0.0):\n",
    "  \"\"\"Saves a complete training checkpoint including model, optimizer, and epoch.\n",
    "\n",
    "  Args:\n",
    "    model: PyTorch model to save.\n",
    "    optimizer: Optimizer state to save.\n",
    "    epoch: Current epoch number.\n",
    "    target_dir: Directory for saving the checkpoint.\n",
    "    checkpoint_name: Filename for the checkpoint. Should include \".pth\" or \".pt\".\n",
    "    scheduler: Optional learning rate scheduler to save.\n",
    "    best_acc: Best validation accuracy achieved so far.\n",
    "\n",
    "  Example usage:\n",
    "    save_checkpoint(model=model, optimizer=optimizer, epoch=20,\n",
    "                    target_dir=\"checkpoints\", checkpoint_name=\"checkpoint_epoch_20.pth\")\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "  target_dir_path = Path(target_dir)\n",
    "  target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "  # Create checkpoint save path\n",
    "  assert checkpoint_name.endswith(\".pth\") or checkpoint_name.endswith(\".pt\"), \\\n",
    "      \"checkpoint_name should end with '.pt' or '.pth'\"\n",
    "  checkpoint_path = target_dir_path / checkpoint_name\n",
    "\n",
    "  # Create checkpoint dictionary\n",
    "  checkpoint = {\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'best_acc': best_acc\n",
    "  }\n",
    "\n",
    "  if scheduler is not None:\n",
    "      checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "  # Save checkpoint\n",
    "  print(f\"[INFO] Saving checkpoint to: {checkpoint_path}\")\n",
    "  torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    checkpoint_path: str,\n",
    "                    scheduler=None,\n",
    "                    device: str = 'cpu'):\n",
    "  \"\"\"Loads a training checkpoint to resume training.\n",
    "\n",
    "  Args:\n",
    "    model: PyTorch model to load weights into.\n",
    "    optimizer: Optimizer to load state into.\n",
    "    checkpoint_path: Path to the checkpoint file.\n",
    "    scheduler: Optional learning rate scheduler to load state into.\n",
    "    device: Device to load the model to.\n",
    "\n",
    "  Returns:\n",
    "    epoch: The epoch number from the checkpoint.\n",
    "    best_acc: The best validation accuracy from the checkpoint.\n",
    "\n",
    "  Example usage:\n",
    "    epoch, best_acc = load_checkpoint(model=model, optimizer=optimizer,\n",
    "                                       checkpoint_path=\"checkpoints/checkpoint_epoch_20.pth\")\n",
    "  \"\"\"\n",
    "  checkpoint_path = Path(checkpoint_path)\n",
    "\n",
    "  if not checkpoint_path.exists():\n",
    "      print(f\"[WARNING] Checkpoint not found at: {checkpoint_path}\")\n",
    "      return 0, 0.0\n",
    "\n",
    "  print(f\"[INFO] Loading checkpoint from: {checkpoint_path}\")\n",
    "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "  if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "  epoch = checkpoint.get('epoch', 0)\n",
    "  best_acc = checkpoint.get('best_acc', 0.0)\n",
    "\n",
    "  print(f\"[INFO] Resumed from epoch {epoch} with best accuracy: {best_acc:.2f}%\")\n",
    "  return epoch, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2535a21",
   "metadata": {},
   "source": [
    "## 04. Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6160327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training engine for model training and validation.\n",
    "This defines the training and validation loops, logging, and checkpointing.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer, device, is_train=True, scheduler=None, step_per_batch=False):\n",
    "    \"\"\"\n",
    "    Single epoch for train or validation.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train or validate\n",
    "        loader: DataLoader for training or validation set\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: torch.device for training or validation\n",
    "        is_train: Whether to train the model\n",
    "        scheduler: Learning rate scheduler\n",
    "        step_per_batch: Whether to step the scheduler per batch\n",
    "\n",
    "    Returns:\n",
    "        running_loss: Average loss for the epoch\n",
    "        correct: Number of correct predictions\n",
    "        total: Total number of samples\n",
    "        accuracy: Accuracy of the model\n",
    "    \"\"\"\n",
    "    model.train() if is_train else model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    desc = \"Train\" if is_train else \"Val\"\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for inputs, targets in tqdm(loader, desc=desc):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler and step_per_batch:\n",
    "                    scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def train_engine(model, train_loader, val_loader, device, epochs=100, lr=0.05,\n",
    "                 weight_decay=5e-4, lr_scheduler='onecycle',\n",
    "                 freeze_backbone=False, unfreeze_epoch=0,\n",
    "                 project_name=\"hackathon\", run_name=None,\n",
    "                 checkpoint_dir=\"checkpoints\", resume_from_checkpoint=True,\n",
    "                 checkpoint_interval=20):\n",
    "    \"\"\"\n",
    "    Main training loop with configurable scheduler.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training set (e.g., 45k images from CIFAR train split)\n",
    "        val_loader: DataLoader for validation set (e.g., 5k images from CIFAR train split)\n",
    "                    Used for monitoring, model selection, and hyperparameter tuning\n",
    "        device: torch.device for training\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay for regularization\n",
    "        lr_scheduler: Learning rate scheduler type ('onecycle', 'cosine', 'step', None)\n",
    "        freeze_backbone: Whether to freeze backbone initially\n",
    "        unfreeze_epoch: Epoch to unfreeze backbone (if freeze_backbone=True)\n",
    "        project_name: WandB project name\n",
    "        run_name: WandB run name\n",
    "        checkpoint_dir: Directory to save/load checkpoints\n",
    "        resume_from_checkpoint: Whether to resume from checkpoint if available\n",
    "        checkpoint_interval: Save checkpoint every N epochs (default: 20)\n",
    "\n",
    "    Returns:\n",
    "        best_acc: Best validation accuracy achieved during training\n",
    "\n",
    "    Note:\n",
    "        - This function uses val_loader for validation during training\n",
    "        - The best model is saved based on validation performance\n",
    "        - DO NOT pass test_loader here - keep test set for final evaluation only!\n",
    "    \"\"\"\n",
    "\n",
    "    wandb.init(project=project_name, name=run_name, config=locals())\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'fc' not in name:  # Assuming final layer is named 'fc'\n",
    "                param.requires_grad = False\n",
    "        print(\"✓ Backbone frozen for initial training.\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # Scheduler selection\n",
    "    if lr_scheduler == 'onecycle':\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=config['lr'], epochs=config['epochs'], steps_per_epoch=len(train_loader)\n",
    "        )\n",
    "        step_per_batch = True\n",
    "    elif lr_scheduler == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        step_per_batch = False\n",
    "    elif lr_scheduler == 'step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        step_per_batch = False\n",
    "    else:\n",
    "        scheduler = None\n",
    "        step_per_batch = False\n",
    "\n",
    "    # Initialize checkpoint directory\n",
    "    checkpoint_path = Path(checkpoint_dir)\n",
    "    checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    start_epoch = 1\n",
    "    best_acc = 0.0\n",
    "    latest_checkpoint = checkpoint_path / \"latest_checkpoint.pth\"\n",
    "\n",
    "    if resume_from_checkpoint and latest_checkpoint.exists():\n",
    "        start_epoch, best_acc = load_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            checkpoint_path=str(latest_checkpoint),\n",
    "            scheduler=scheduler,\n",
    "            device=device\n",
    "        )\n",
    "        start_epoch += 1  # Start from next epoch\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        # Unfreeze backbone if specified\n",
    "        if unfreeze_epoch > 0 and epoch == unfreeze_epoch:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=weight_decay)\n",
    "            print(f\"Backbone unfrozen at epoch {epoch} with reduced Learning Rate.\")\n",
    "            wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device, True, scheduler, step_per_batch)\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, None, device, False)\n",
    "\n",
    "        wandb.log({\n",
    "            'train/loss': train_loss, 'train/acc': train_acc,\n",
    "            'val/loss': val_loss, 'val/acc': val_acc,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "        # Step scheduler per epoch if not per batch\n",
    "        if scheduler and not step_per_batch:\n",
    "            scheduler.step()\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Best model saved: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save checkpoint every checkpoint_interval epochs\n",
    "        if epoch % checkpoint_interval == 0:\n",
    "            save_checkpoint(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch,\n",
    "                target_dir=checkpoint_dir,\n",
    "                checkpoint_name=f\"checkpoint_epoch_{epoch}.pth\",\n",
    "                scheduler=scheduler,\n",
    "                best_acc=best_acc\n",
    "            )\n",
    "            # Also save as latest checkpoint for easy resuming\n",
    "            save_checkpoint(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch,\n",
    "                target_dir=checkpoint_dir,\n",
    "                checkpoint_name=\"latest_checkpoint.pth\",\n",
    "                scheduler=scheduler,\n",
    "                best_acc=best_acc\n",
    "            )\n",
    "\n",
    "    wandb.finish()\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a847acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full(config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Get data\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        dataset_name=config['dataset'],\n",
    "        batch_size=config['batch_size'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory'],\n",
    "        use_pretrained=config['use_pretrained'],\n",
    "        pretrained_model_name=config['pretrained_model_name']\n",
    "    )\n",
    "\n",
    "    # Get model\n",
    "    model = get_model(config['model'], config['num_classes'], config['dropout'], config['use_pretrained'])\n",
    "        # 🔍 DEBUG: Check model output shape and labels\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DEBUG INFO\")\n",
    "    print(\"=\"*60)\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sample_inputs, sample_labels = sample_batch\n",
    "    print(f\"Sample input shape: {sample_inputs.shape}\")\n",
    "    print(f\"Sample labels shape: {sample_labels.shape}\")\n",
    "    print(f\"Label range: [{sample_labels.min()}, {sample_labels.max()}]\")\n",
    "    print(f\"Config num_classes: {config['num_classes']}\")\n",
    "\n",
    "    # Test forward pass on CPU first\n",
    "    model = get_model(config['model'], config['num_classes'], config['dropout'], config['use_pretrained'])\n",
    "    test_output = model_cpu(sample_inputs[:2])  # Just 2 samples\n",
    "    print(f\"Model output shape: {test_output.shape}\")\n",
    "    print(f\"Expected: (2, {config['num_classes']})\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Train\n",
    "    best_acc = train_engine(\n",
    "        model, train_loader, val_loader, device,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['lr'],\n",
    "        lr_scheduler=config['lr_scheduler'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        project_name=config['project'],\n",
    "        run_name=config['run_name'],\n",
    "\n",
    "        freeze_backbone=config['freeze_backbone'],\n",
    "        unfreeze_epoch=config['freeze_epochs'],\n",
    "\n",
    "        checkpoint_dir=config['checkpoint_dir'],\n",
    "        resume_from_checkpoint=config['resume_from_checkpoint'],\n",
    "        checkpoint_interval=config['checkpoint_interval']\n",
    "    )\n",
    "\n",
    "    print(f\"Training complete! Best accuracy: {best_acc:.2f}%\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "============================================================\n",
      "[Transfer Learning Mode]\n",
      "Model: vit_base_patch16_224\n",
      "Input size: 224x224\n",
      "Statistics: ImageNet (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "Augmentation: Strong (auto_augment, flip, erase)\n",
      "============================================================\n",
      "\n",
      "Dataset: CIFAR100\n",
      "Mean/Std: (0.485, 0.456, 0.406) / (0.229, 0.224, 0.225)\n",
      "Train: 45000, Val: 5000, Test: 10000\n",
      "Batch size: 256, Num workers: 4, Pin memory: True\n",
      "\n",
      "\n",
      "============================================================\n",
      "DEBUG INFO\n",
      "============================================================\n",
      "Sample input shape: torch.Size([256, 3, 224, 224])\n",
      "Sample labels shape: torch.Size([256])\n",
      "Label range: [0, 99]\n",
      "Config num_classes: 100\n"
     ]
    }
   ],
   "source": [
    "model = train_full(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148933a",
   "metadata": {},
   "source": [
    "## 05. Evaluate w Test Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b103342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model evaluation with essential metrics\n",
    "\"\"\"\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, test_loader, device, class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate model and return comprehensive metrics.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model to evaluate\n",
    "        test_loader: DataLoader for test set (use ONLY for final evaluation!)\n",
    "        device: torch.device for evaluation\n",
    "        class_names: List of class names for classification report\n",
    "\n",
    "    Returns:\n",
    "        dict: {'accuracy',           # Overall accuracy\n",
    "               'f1_macro',           # f1 score for macro average\n",
    "               'f1_weighted',        # f1 score for weighted average\n",
    "               'confusion_matrix',   # Confusion matrix\n",
    "               'per_class_acc',      # Per-class accuracy\n",
    "               'classification_report',\n",
    "               'predictions',        # Predicted labels\n",
    "               'targets',            # True labels\n",
    "\n",
    "    Note:\n",
    "        - Use this function ONLY for final test set evaluation\n",
    "        - Load the best model checkpoint before calling this\n",
    "        - Do NOT use this during training/validation - it should be called once at the end\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = 100. * (all_preds == all_targets).sum() / len(all_targets)\n",
    "    f1_macro = f1_score(all_targets, all_preds, average='macro')\n",
    "    f1_weighted = f1_score(all_targets, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    # Per-class accuracy\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        all_targets, all_preds,\n",
    "        target_names=class_names,\n",
    "        digits=3\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.3f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.3f}\")\n",
    "    print(f\"\\n{report}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class_acc': per_class_acc,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ce9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_loader = get_dataloaders(\n",
    "        dataset_name=config['dataset'],\n",
    "        batch_size=config['batch_size'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory'],\n",
    "        use_pretrained=config['use_pretrained'],\n",
    "        pretrained_model_name=config['pretrained_model_name']\n",
    "    )\n",
    "\n",
    "eval_dict = evaluate(model, test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab72b68",
   "metadata": {},
   "source": [
    "## 06. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013968aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization utilities using wandb\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "def _require_run():\n",
    "    if wandb.run is None:\n",
    "        raise RuntimeError(\"No active W&B run. Call wandb.init(...) before logging.\")\n",
    "\n",
    "def log_confusion_matrix_from_matrix(cm, class_names, title=\"Confusion Matrix\"):\n",
    "    _require_run()\n",
    "    df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    wandb.log({\n",
    "        \"confusion_matrix\": wandb.plot.heatmap(\n",
    "            df,\n",
    "            x_label=\"Predicted\",\n",
    "            y_label=\"True\",\n",
    "            title=title\n",
    "        )\n",
    "    })\n",
    "\n",
    "def log_per_class_metrics(per_class_acc, class_names):\n",
    "    \"\"\"\n",
    "    Log per-class accuracy as wandb bar chart\n",
    "\n",
    "    Args:\n",
    "        per_class_acc: array of per-class accuracies\n",
    "        class_names: list of class names\n",
    "    \"\"\"\n",
    "    _require_run()\n",
    "    # Create bar chart data\n",
    "    data = [[class_names[i], per_class_acc[i] * 100] for i in range(len(class_names))]\n",
    "    table = wandb.Table(data=data, columns=[\"Class\", \"Accuracy (%)\"])\n",
    "\n",
    "    wandb.log({\n",
    "        \"per_class_accuracy\": wandb.plot.bar(\n",
    "            table, \"Class\", \"Accuracy (%)\",\n",
    "            title=\"Per-Class Accuracy\"\n",
    "        )\n",
    "    })\n",
    "    print(\"✓ Per-class accuracy logged to wandb\")\n",
    "\n",
    "def log_sample_predictions(model, test_loader, device, class_names, num_samples=16):\n",
    "    \"\"\"\n",
    "    Log sample predictions to wandb\n",
    "\n",
    "    Args:\n",
    "        model: trained model\n",
    "        test_loader: test dataloader\n",
    "        device: torch device\n",
    "        class_names: list of class names\n",
    "        num_samples: number of samples to log\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get one batch\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images[:num_samples].to(device), labels[:num_samples]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "    # Create wandb images with predictions\n",
    "    _require_run()\n",
    "    wandb_images = []\n",
    "    for idx in range(num_samples):\n",
    "        img = images[idx].cpu()\n",
    "        true_label = class_names[labels[idx]]\n",
    "        pred_label = class_names[predicted[idx].cpu()]\n",
    "        pred_prob = probs[idx][predicted[idx]].cpu().item()\n",
    "\n",
    "        caption = f\"True: {true_label} | Pred: {pred_label} ({pred_prob:.2%})\"\n",
    "        wandb_images.append(wandb.Image(img, caption=caption))\n",
    "\n",
    "    wandb.log({\"predictions\": wandb_images})\n",
    "    print(f\"✓ {num_samples} sample predictions logged to wandb\")\n",
    "\n",
    "def log_all_metrics(results, class_names):\n",
    "    \"\"\"\n",
    "    Log all evaluation metrics to wandb\n",
    "\n",
    "    Args:\n",
    "        results: dict from evaluate() function\n",
    "        class_names: list of class names\n",
    "    \"\"\"\n",
    "    _require_run()\n",
    "    # Log summary metrics\n",
    "    wandb.log({\n",
    "        \"test/accuracy\": results['accuracy'],\n",
    "        \"test/f1_macro\": results['f1_macro'],\n",
    "        \"test/f1_weighted\": results['f1_weighted'],\n",
    "    })\n",
    "\n",
    "    # Log per-class accuracies\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        wandb.log({f\"test/accuracy_{class_name}\": results['per_class_acc'][i] * 100})\n",
    "\n",
    "    print(\"✓ All metrics logged to wandb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
