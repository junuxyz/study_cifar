{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f09fbd",
   "metadata": {},
   "source": [
    "## 01. Configuration For Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218d0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # ========================\n",
    "    # Dataset\n",
    "    # ========================\n",
    "    'dataset': 'CIFAR100',\n",
    "    'num_classes': 100,\n",
    "    'batch_size': 256,\n",
    "    'val_ratio': 0.1,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "\n",
    "    # ========================\n",
    "    # Transfer Learning\n",
    "    # ========================\n",
    "    # use_pretrained=TrueÏù¥Î©¥ ÏïÑÎûò ÏÑ§Ï†ïÎì§Ïù¥ ÏûêÎèôÏúºÎ°ú Í≤∞Ï†ïÎê®:\n",
    "    #   - mean/std: ImageNet ÏûêÎèô Ï†ÅÏö©\n",
    "    #   - input_size: Î™®Îç∏ÏóêÏÑú ÏûêÎèô Í≤∞Ï†ï\n",
    "    #   - strong_aug: True (ÏûêÎèô)\n",
    "    #   - augmentation: bicubic resize, auto_augment Ìè¨Ìï®\n",
    "    'use_pretrained': True,  # TrueÎ©¥ Transfer Learning Î™®Îìú\n",
    "    'pretrained_model_name': 'vit_base_patch16_224',  # timm Î™®Îç∏ Ïù¥Î¶Ñ\n",
    "    \n",
    "    # Supported pretrained models (timm):\n",
    "    #  - 'resnet50', 'resnet101': Classical ResNet\n",
    "    #  - 'convnext_small', 'convnext_base': Modern ConvNet\n",
    "    #  - 'vit_base_patch16_224': Vision Transformer\n",
    "    #  - 'deit_base_patch16_224': Data-efficient ViT\n",
    "    #  - 'vit_large_patch14_224_in21k': Larger ViT\n",
    "\n",
    "    # ========================\n",
    "    # Augmentation (use_pretrained=FalseÏùº ÎïåÎßå Ï†ÅÏö©)\n",
    "    # ========================\n",
    "    'strong_aug': False,\n",
    "    'flip': True,\n",
    "    'erase': False,\n",
    "    'color_jitter': False,\n",
    "\n",
    "    # ========================\n",
    "    # Model Architecture\n",
    "    # ========================\n",
    "    'model': 'convnext_s',\n",
    "    'num_classes': 100,\n",
    "    'dropout': 0.0,\n",
    "\n",
    "    # ========================\n",
    "    # Training\n",
    "    # ========================\n",
    "    'epochs': 1,\n",
    "    'lr': 0.001,\n",
    "    'lr_scheduler': 'cosine',  # 'step', 'cosine', 'onecycle', 'exponential'\n",
    "    'lr_decay_factor': 0.9,    # Multiplicative decay when lr_scheduler='exponential'\n",
    "    'weight_decay': 1e-4,\n",
    "\n",
    "    # ========================\n",
    "    # Fine-tuning Strategy\n",
    "    # ========================\n",
    "    # Transfer LearningÏóêÏÑú Ï§ëÏöîÌïú ÏÑ§Ï†ï\n",
    "    'freeze_backbone': True,       # Ï≤òÏùåÏóê backbone freezeÌï†ÏßÄ\n",
    "    'freeze_epochs': 10,           # N epoch ÎèôÏïà backbone freeze Ïú†ÏßÄ\n",
    "    'unfreeze_strategy': 'gradual', # 'gradual': epochÎßàÎã§ unfreeze, 'all_at_once': freeze_epochs ÌõÑ ÌïúÎ≤àÏóê\n",
    "    \n",
    "    # Gradual unfreezing Í¥ÄÎ†®\n",
    "    'unfreeze_every_n_epochs': 5,  # 5 epochÎßàÎã§ Ìïú Ï∏µÏî© unfreeze\n",
    "    'layer_lr_decay': 0.1,         # Îí§Î°ú Í∞àÏàòÎ°ù learning rate 0.1Î∞∞Ïî© Í∞êÏÜå\n",
    "\n",
    "    # ========================\n",
    "    # Checkpointing\n",
    "    # ========================\n",
    "    'checkpoint_dir': 'checkpoints',\n",
    "    'resume_from_checkpoint': False,\n",
    "    'checkpoint_interval': 10,\n",
    "\n",
    "    # ========================\n",
    "    # Logging\n",
    "    # ========================\n",
    "    'project': 'transfer_learning',\n",
    "    'run_name': None,  # Auto-generated if None\n",
    "    'log_interval': 50,  # Log every N batches\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9b252",
   "metadata": {},
   "source": [
    "## 02. Load and Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0e796c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpotw/Projects/study/study_cifar/.venv/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/jpotw/Projects/study/study_cifar/.venv/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from typing import Optional\n",
    "import timm\n",
    "from timm.data import create_transform\n",
    "\n",
    "DEFAULT_STATS = {\n",
    "    \"CIFAR10\":  ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    \"CIFAR100\": ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "    \"FashionMNIST\": ((0.2860,), (0.3530,)),\n",
    "}\n",
    "\n",
    "def get_class_names(dataset_name, root=\"./data\", train=True):\n",
    "    \"\"\"Return list of class label names for a torchvision dataset\"\"\"\n",
    "    ds_class = getattr(torchvision.datasets, dataset_name)\n",
    "    dataset = ds_class(root=root, train=train, download=True)\n",
    "    if hasattr(dataset, \"classes\"):\n",
    "        return dataset.classes\n",
    "    elif hasattr(dataset, \"labels\"):\n",
    "        return dataset.labels\n",
    "    else:\n",
    "        raise AttributeError(f\"{dataset_name} has no attribute 'classes' or 'labels'\")\n",
    "\n",
    "def compute_mean_std(dataset):\n",
    "    \"\"\"ToTensor()Îßå Ï†ÅÏö©Îêú datasetÏóêÏÑú Ï±ÑÎÑêÎ≥Ñ mean/std Í≥ÑÏÇ∞\"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=1024, shuffle=False)\n",
    "    n = 0\n",
    "    s1, s2 = 0.0, 0.0\n",
    "    for x, _ in loader:\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, -1)\n",
    "        n += b * h * w\n",
    "        s1 += x.sum(dim=(0, 2))\n",
    "        s2 += (x ** 2).sum(dim=(0, 2))\n",
    "    mean = s1 / n\n",
    "    std = torch.sqrt(s2 / n - mean ** 2)\n",
    "    return tuple(mean.tolist()), tuple(std.tolist())\n",
    "\n",
    "def get_transforms(image_size, mean, std, *, \n",
    "                  strong_aug=True, flip=True,\n",
    "                  erase=False, pretrained_input_size=None, use_pretrained=False):\n",
    "    \"\"\"\n",
    "    Transform ÏÉùÏÑ±.\n",
    "    \n",
    "    Args:\n",
    "        use_pretrained: TrueÎ©¥ timmÏùò ÏµúÏ†ÅÌôîÎêú transform ÏÇ¨Ïö© (ImageNet Í∏∞Ï§Ä)\n",
    "        pretrained_input_size: Pretrained Î™®Îç∏Ïùò input size\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pretrained Î™®Îç∏Ïö© ÏµúÏ†ÅÌôîÎêú transform (timm)\n",
    "    if use_pretrained and pretrained_input_size is not None:\n",
    "        transform_train = create_transform(\n",
    "            input_size=pretrained_input_size,\n",
    "            is_training=True,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "            auto_augment='rand-m9-mstd0.5' if strong_aug else None,\n",
    "            interpolation='bicubic',\n",
    "            hflip=flip,\n",
    "            re_prob=0.25 if erase else 0.0,\n",
    "        )\n",
    "        transform_test = create_transform(\n",
    "            input_size=pretrained_input_size,\n",
    "            is_training=False,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "            interpolation='bicubic',\n",
    "        )\n",
    "        return transform_train, transform_test\n",
    "    \n",
    "    # ÏùºÎ∞ò ÌïôÏäµÏö© transform\n",
    "    train_trans = []\n",
    "    test_trans = []\n",
    "    \n",
    "    if pretrained_input_size is not None:\n",
    "        train_trans.append(T.Resize(pretrained_input_size, interpolation=T.InterpolationMode.BICUBIC))\n",
    "        test_trans.append(T.Resize(pretrained_input_size, interpolation=T.InterpolationMode.BICUBIC))\n",
    "    else:\n",
    "        train_trans.append(T.RandomCrop(image_size, padding=4))\n",
    "    \n",
    "    if flip:\n",
    "        train_trans.append(T.RandomHorizontalFlip())\n",
    "    if strong_aug:\n",
    "        train_trans.insert(0, T.TrivialAugmentWide())\n",
    "    \n",
    "    train_trans += [T.ToTensor(), T.Normalize(mean, std)]\n",
    "    test_trans += [T.ToTensor(), T.Normalize(mean, std)]\n",
    "    \n",
    "    if erase:\n",
    "        train_trans.append(T.RandomErasing(p=0.5))\n",
    "    \n",
    "    return T.Compose(train_trans), T.Compose(test_trans)\n",
    "\n",
    "def get_dataloaders(\n",
    "    dataset_name: str = \"CIFAR10\",\n",
    "    batch_size: int = 128,\n",
    "    data_root: str = \"./data\",\n",
    "    val_ratio: float = 0.1,\n",
    "    num_workers: int = 4,\n",
    "    pin_memory: bool = True,\n",
    "    use_pretrained: bool = False,\n",
    "    pretrained_model_name: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    train_loader, val_loader, test_loader ÏûêÎèô Íµ¨ÏÑ±.\n",
    "    \n",
    "    Transfer Learning ÏÇ¨Ïö© Ïãú:\n",
    "        use_pretrained=True Î°ú ÏÑ§Ï†ïÌïòÎ©¥ Î™®Îì† ÏÑ§Ï†ïÏù¥ ÏûêÎèôÏúºÎ°ú Ï†ÅÏö©Îê®\n",
    "        - ImageNet ÌÜµÍ≥Ñ (mean/std)\n",
    "        - timm ÏµúÏ†ÅÌôî transform (bicubic resize, augmentation)\n",
    "        - Ï†ÅÏ†àÌïú augmentation (Í∞ïÌï®)\n",
    "    \n",
    "    Args:\n",
    "        use_pretrained: Pretrained Î™®Îç∏ ÏÇ¨Ïö© Ïó¨Î∂Ä\n",
    "        pretrained_model_name: timm Î™®Îç∏ Ïù¥Î¶Ñ (Ïòà: 'resnet50', 'vit_base_patch16_224')\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================\n",
    "    # Transfer Learning ÏÑ§Ï†ï\n",
    "    # ========================\n",
    "    if use_pretrained:\n",
    "        # timmÏóêÏÑú Î™®Îç∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        try:\n",
    "            model_info = timm.get_model_pretrained_cfg(pretrained_model_name)\n",
    "            pretrained_input_size = model_info.input_size[-1]  # (3, 224, 224) ‚Üí 224\n",
    "        except:\n",
    "            # Î™®Îç∏ Ï†ïÎ≥¥Î•º Î™ª Í∞ÄÏ†∏Ïò® Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í\n",
    "            pretrained_input_size = 224\n",
    "        \n",
    "        # ImageNet ÌÜµÍ≥Ñ ÏûêÎèô Ï†ÅÏö©\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        \n",
    "        # PretrainedÎäî Ìï≠ÏÉÅ Í∞ïÌïú augmentation Ï†ÅÏö©\n",
    "        strong_aug = True\n",
    "        flip = True\n",
    "        erase = True\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[Transfer Learning Mode]\")\n",
    "        print(f\"Model: {pretrained_model_name}\")\n",
    "        print(f\"Input size: {pretrained_input_size}x{pretrained_input_size}\")\n",
    "        print(f\"Statistics: ImageNet (mean={mean}, std={std})\")\n",
    "        print(f\"Augmentation: Strong (auto_augment, flip, erase)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # ========================\n",
    "    # ÏùºÎ∞ò ÌïôÏäµ ÏÑ§Ï†ï\n",
    "    # ========================\n",
    "    else:\n",
    "        pretrained_input_size = None\n",
    "        strong_aug = True\n",
    "        flip = True\n",
    "        erase = False\n",
    "        \n",
    "        if dataset_name in DEFAULT_STATS:\n",
    "            mean, std = DEFAULT_STATS[dataset_name]\n",
    "        else:\n",
    "            tmp = getattr(torchvision.datasets, dataset_name)(\n",
    "                root=data_root, train=True, download=True, transform=T.ToTensor()\n",
    "            )\n",
    "            mean, std = compute_mean_std(tmp)\n",
    "        \n",
    "        print(f\"\\n[Standard Training Mode] Dataset: {dataset_name}\")\n",
    "    \n",
    "    # Transform ÏÉùÏÑ±\n",
    "    image_size = pretrained_input_size if pretrained_input_size else 32\n",
    "    transform_train, transform_test = get_transforms(\n",
    "        image_size, mean, std,\n",
    "        strong_aug=strong_aug,\n",
    "        flip=flip,\n",
    "        erase=erase,\n",
    "        pretrained_input_size=pretrained_input_size,\n",
    "        use_pretrained=config['use_pretrained'],\n",
    "    )\n",
    "    \n",
    "    # Dataset Î°úÎìú\n",
    "    ds_class = getattr(torchvision.datasets, dataset_name)\n",
    "    full_train = ds_class(root=data_root, train=True, download=True, transform=transform_train)\n",
    "    test_set = ds_class(root=data_root, train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    # Validation split\n",
    "    val_size = int(len(full_train) * val_ratio)\n",
    "    train_size = len(full_train) - val_size\n",
    "    train_set, val_set = random_split(full_train, [train_size, val_size])\n",
    "    \n",
    "    # DataLoader ÏÉùÏÑ±\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=config['batch_size'], shuffle=True,\n",
    "        num_workers=config['num_workers'], pin_memory=config['pin_memory'], persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=config['batch_size'], shuffle=False,\n",
    "        num_workers=config['num_workers'], pin_memory=config['pin_memory'], persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=config['batch_size'], shuffle=False,\n",
    "        num_workers=config['num_workers'], pin_memory=config['pin_memory'], persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Mean/Std: {mean} / {std}\")\n",
    "    print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n",
    "    print(f\"Batch size: {batch_size}, Num workers: {num_workers}, Pin memory: {pin_memory}\\n\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e73ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# WideResNet (from scratch)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout_rate=0.0, stride=1):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_ch)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.shortcut = (\n",
    "            nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)\n",
    "            if (stride != 1 or in_ch != out_ch) else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        x = self.conv1(self.relu1(self.bn1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(self.relu2(self.bn2(x)))\n",
    "        return x + res\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, widen_factor=10, dropout_rate=0.3, num_classes=10):\n",
    "        super().__init__()\n",
    "        assert ((depth - 4) % 6 == 0), \"depth should be 6n+4\"\n",
    "        N = (depth - 4) // 6\n",
    "        K = widen_factor\n",
    "\n",
    "        self.stem = nn.Conv2d(3, 16, 3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_group(16, 16*K, N, dropout_rate, stride=1)\n",
    "        self.layer2 = self._make_group(16*K, 32*K, N, dropout_rate, stride=2)\n",
    "        self.layer3 = self._make_group(32*K, 64*K, N, dropout_rate, stride=2)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm2d(64*K),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*K, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_group(self, in_ch, out_ch, N, dropout_rate, stride):\n",
    "        layers = [Block(in_ch, out_ch, dropout_rate, stride)]\n",
    "        for _ in range(1, N):\n",
    "            layers.append(Block(out_ch, out_ch, dropout_rate, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "def _make_resnet18_cifar(num_classes=10, pretrained=False, dropout=0.0):\n",
    "    if pretrained:\n",
    "        weights = torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "    else:\n",
    "        weights = None\n",
    "    m = torchvision.models.resnet18(weights=weights)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    in_feat = m.fc.in_features\n",
    "    head = [nn.Linear(in_feat, num_classes)]\n",
    "    if dropout > 0:\n",
    "        # Create a list of layers for the head\n",
    "        head = [nn.Dropout(dropout), nn.Linear(in_feat, num_classes)]\n",
    "    # Use nn.Sequential to combine the layers in the head\n",
    "    m.fc = nn.Sequential(*head)\n",
    "    return m\n",
    "\n",
    "\n",
    "def get_model(model_name, num_classes=10, dropout=0.0, pretrained=True):\n",
    "    \"\"\"\n",
    "    Supported models:\n",
    "      - 'wrn28_10': good for cifar-10/100 (only when pixels are 32x32 or 64x64)\n",
    "      - 'convnext_s': small gpu, fast baseline\n",
    "      - 'vit_b16_aug': good accuracy, moderate gpu\n",
    "      - 'deit_b16d': good accuracy, moderate gpu\n",
    "      - 'eva_b14': optional heavy\n",
    "      - 'resnet18_cifar': classic baseline for cifar-10/100\n",
    "    \"\"\"\n",
    "    name = model_name.lower()\n",
    "    timm_map = {\n",
    "        'convnext_s':  'convnext_small.fb_in22k_ft_in1k',\n",
    "        'deit_b16d':   'deit_base_distilled_patch16_224',\n",
    "        'vit_b16_aug': 'vit_base_patch16_224.augreg_in21k_ft_in1k',\n",
    "        'eva_b14':     'eva02_base_patch14_224',\n",
    "    }\n",
    "\n",
    "    if name in timm_map:\n",
    "        # timmÏùÄ drop_rate, drop_path_rate Îëò Îã§ ÏßÄÏõê. Ïª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ dpr 0.1~0.2 Ï∂îÏ≤ú.\n",
    "        return timm.create_model(\n",
    "            timm_map[name],\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            drop_rate=dropout,\n",
    "            drop_path_rate=0.1\n",
    "        )\n",
    "    if name == 'wrn28_10':\n",
    "        return WideResNet(depth=28, widen_factor=10, dropout_rate=dropout, num_classes=num_classes)\n",
    "\n",
    "    if name == 'resnet18_cifar':\n",
    "        return _make_resnet18_cifar(num_classes=num_classes, pretrained=pretrained, dropout=dropout)\n",
    "\n",
    "    raise ValueError(f\"Unknown model: {model_name}. Choose from \"\n",
    "                     f\"['wrn28_10','convnext_s','deit_b16d','vit_b16_aug','eva_b14','resnet18_cifar'].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df556eb",
   "metadata": {},
   "source": [
    "### 3-1. Helper Functions for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3102a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# 1) output fc layer label ÎßûÏ∂îÍ∏∞\n",
    "def replace_classifier(model, num_classes: int) -> None:\n",
    "    \"\"\"\n",
    "    Î™®Îç∏Ïùò ÏµúÏ¢Ö Î∂ÑÎ•ò Ìó§ÎìúÎ•º num_classesÏóê ÎßûÍ≤å ÍµêÏ≤¥.\n",
    "    torchvision(fc), timm(classifier/head) Î™®Îëê ÎåÄÏùë.\n",
    "    \"\"\"\n",
    "    # torchvision: resnet Îì±\n",
    "    if hasattr(model, \"fc\") and isinstance(model.fc, nn.Linear):\n",
    "        in_f = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_f, num_classes)\n",
    "        return\n",
    "    # timm: efficientnet Îì±\n",
    "    if hasattr(model, \"classifier\") and isinstance(model.classifier, nn.Linear):\n",
    "        in_f = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(in_f, num_classes)\n",
    "        return\n",
    "    # timm: convnext Îì±\n",
    "    if hasattr(model, \"head\") and isinstance(model.head, nn.Linear):\n",
    "        in_f = model.head.in_features\n",
    "        model.head = nn.Linear(in_f, num_classes)\n",
    "        return\n",
    "    raise RuntimeError(\"No replaceable classifier head found (fc/classifier/head).\")\n",
    "\n",
    "# 2) freeze Ï†ïÎèÑ (Î∞±Î≥∏Îßå ÏñºÎ¶¨Í∏∞ or Ìï¥Ï†ú)\n",
    "def apply_freeze(model, freeze_backbone: bool, head_names=(\"fc\",\"classifier\",\"head\")) -> None:\n",
    "    \"\"\"\n",
    "    freeze_backbone=TrueÎ©¥ Ìó§ÎìúÎßå ÌïôÏäµ, FalseÎ©¥ Ï†ÑÎ∂Ä ÌïôÏäµ.\n",
    "    \"\"\"\n",
    "    for name, p in model.named_parameters():\n",
    "        p.requires_grad = (not freeze_backbone) or any(h in name for h in head_names)\n",
    "\n",
    "# 3) lr (head) Î≥ÄÌôî: head(ÌïôÏäµÌïòÎäî Î∂ÄÎ∂Ñ)/backbone(Í≥†Ï†ïÌïòÎäî Î∂ÄÎ∂Ñ) ÏÑúÎ°ú Îã§Î•∏ LRÏö© ÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î£π ÎßåÎì§Í∏∞\n",
    "def make_param_groups(model, lr_head: float, lr_backbone: float, weight_decay: float = 5e-4, momentum: float = 0.9,\n",
    "                      head_names=(\"fc\",\"classifier\",\"head\")):\n",
    "    \"\"\"\n",
    "    optimizerÏóê Î∞îÎ°ú ÎÑ£ÏùÑ Ïàò ÏûàÎäî param_groups Î∞òÌôò.\n",
    "    SGD/AdamW Îì± Ïñ¥Îñ§ ÏòµÌã∞ÎßàÏù¥Ï†ÄÏóêÎèÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö© Í∞ÄÎä•.\n",
    "    \"\"\"\n",
    "    head_params, backbone_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        (head_params if any(h in n for h in head_names) else backbone_params).append(p)\n",
    "\n",
    "    pg = []\n",
    "    if backbone_params:\n",
    "        pg.append(dict(params=backbone_params, lr=lr_backbone, weight_decay=config['weight_decay'], momentum=momentum))\n",
    "    if head_params:\n",
    "        pg.append(dict(params=head_params, lr=lr_head, weight_decay=config['weight_decay'], momentum=momentum))\n",
    "    return pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add93641",
   "metadata": {},
   "source": [
    "### 03-2. Save and Load Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d49b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "  \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "  Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "\n",
    "  Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "  target_dir_path = Path(target_dir)\n",
    "  target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "  # Create model save path\n",
    "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "  model_save_path = target_dir_path / model_name\n",
    "\n",
    "  # Save the model state_dict()\n",
    "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "  torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)\n",
    "\n",
    "\n",
    "def save_checkpoint(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    epoch: int,\n",
    "                    target_dir: str,\n",
    "                    checkpoint_name: str,\n",
    "                    scheduler=None,\n",
    "                    best_acc: float = 0.0):\n",
    "  \"\"\"Saves a complete training checkpoint including model, optimizer, and epoch.\n",
    "\n",
    "  Args:\n",
    "    model: PyTorch model to save.\n",
    "    optimizer: Optimizer state to save.\n",
    "    epoch: Current epoch number.\n",
    "    target_dir: Directory for saving the checkpoint.\n",
    "    checkpoint_name: Filename for the checkpoint. Should include \".pth\" or \".pt\".\n",
    "    scheduler: Optional learning rate scheduler to save.\n",
    "    best_acc: Best validation accuracy achieved so far.\n",
    "\n",
    "  Example usage:\n",
    "    save_checkpoint(model=model, optimizer=optimizer, epoch=20,\n",
    "                    target_dir=\"checkpoints\", checkpoint_name=\"checkpoint_epoch_20.pth\")\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "  target_dir_path = Path(target_dir)\n",
    "  target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "  # Create checkpoint save path\n",
    "  assert checkpoint_name.endswith(\".pth\") or checkpoint_name.endswith(\".pt\"), \\\n",
    "      \"checkpoint_name should end with '.pt' or '.pth'\"\n",
    "  checkpoint_path = target_dir_path / checkpoint_name\n",
    "\n",
    "  # Create checkpoint dictionary\n",
    "  checkpoint = {\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'best_acc': best_acc\n",
    "  }\n",
    "\n",
    "  if scheduler is not None:\n",
    "      checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "  # Save checkpoint\n",
    "  print(f\"[INFO] Saving checkpoint to: {checkpoint_path}\")\n",
    "  torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    checkpoint_path: str,\n",
    "                    scheduler=None,\n",
    "                    device: str = 'cpu'):\n",
    "  \"\"\"Loads a training checkpoint to resume training.\n",
    "\n",
    "  Args:\n",
    "    model: PyTorch model to load weights into.\n",
    "    optimizer: Optimizer to load state into.\n",
    "    checkpoint_path: Path to the checkpoint file.\n",
    "    scheduler: Optional learning rate scheduler to load state into.\n",
    "    device: Device to load the model to.\n",
    "\n",
    "  Returns:\n",
    "    epoch: The epoch number from the checkpoint.\n",
    "    best_acc: The best validation accuracy from the checkpoint.\n",
    "\n",
    "  Example usage:\n",
    "    epoch, best_acc = load_checkpoint(model=model, optimizer=optimizer,\n",
    "                                       checkpoint_path=\"checkpoints/checkpoint_epoch_20.pth\")\n",
    "  \"\"\"\n",
    "  checkpoint_path = Path(checkpoint_path)\n",
    "\n",
    "  if not checkpoint_path.exists():\n",
    "      print(f\"[WARNING] Checkpoint not found at: {checkpoint_path}\")\n",
    "      return 0, 0.0\n",
    "\n",
    "  print(f\"[INFO] Loading checkpoint from: {checkpoint_path}\")\n",
    "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "  if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "  epoch = checkpoint.get('epoch', 0)\n",
    "  best_acc = checkpoint.get('best_acc', 0.0)\n",
    "\n",
    "  print(f\"[INFO] Resumed from epoch {epoch} with best accuracy: {best_acc:.2f}%\")\n",
    "  return epoch, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2535a21",
   "metadata": {},
   "source": [
    "## 04. Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6160327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training engine for model training and validation.\n",
    "This defines the training and validation loops, logging, and checkpointing.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer, device, is_train=True, scheduler=None, step_per_batch=False):\n",
    "    \"\"\"\n",
    "    Single epoch for train or validation.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train or validate\n",
    "        loader: DataLoader for training or validation set\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: torch.device for training or validation\n",
    "        is_train: Whether to train the model\n",
    "        scheduler: Learning rate scheduler\n",
    "        step_per_batch: Whether to step the scheduler per batch\n",
    "\n",
    "    Returns:\n",
    "        running_loss: Average loss for the epoch\n",
    "        correct: Number of correct predictions\n",
    "        total: Total number of samples\n",
    "        accuracy: Accuracy of the model\n",
    "    \"\"\"\n",
    "    model.train() if is_train else model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    desc = \"Train\" if is_train else \"Val\"\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for inputs, targets in tqdm(loader, desc=desc):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler and step_per_batch:\n",
    "                    scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def train_engine(model, train_loader, val_loader, device, epochs=100, lr=0.05,\n",
    "                 weight_decay=5e-4, lr_scheduler='onecycle',\n",
    "                 freeze_backbone=False, unfreeze_epoch=0,\n",
    "                 project_name=\"hackathon\", run_name=None,\n",
    "                 checkpoint_dir=\"checkpoints\", resume_from_checkpoint=True,\n",
    "                 checkpoint_interval=20):\n",
    "    \"\"\"\n",
    "    Main training loop with configurable scheduler.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training set (e.g., 45k images from CIFAR train split)\n",
    "        val_loader: DataLoader for validation set (e.g., 5k images from CIFAR train split)\n",
    "                    Used for monitoring, model selection, and hyperparameter tuning\n",
    "        device: torch.device for training\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay for regularization\n",
    "        lr_scheduler: Learning rate scheduler type ('onecycle', 'cosine', 'step', None)\n",
    "        freeze_backbone: Whether to freeze backbone initially\n",
    "        unfreeze_epoch: Epoch to unfreeze backbone (if freeze_backbone=True)\n",
    "        project_name: WandB project name\n",
    "        run_name: WandB run name\n",
    "        checkpoint_dir: Directory to save/load checkpoints\n",
    "        resume_from_checkpoint: Whether to resume from checkpoint if available\n",
    "        checkpoint_interval: Save checkpoint every N epochs (default: 20)\n",
    "\n",
    "    Returns:\n",
    "        best_acc: Best validation accuracy achieved during training\n",
    "\n",
    "    Note:\n",
    "        - This function uses val_loader for validation during training\n",
    "        - The best model is saved based on validation performance\n",
    "        - DO NOT pass test_loader here - keep test set for final evaluation only!\n",
    "    \"\"\"\n",
    "\n",
    "    wandb.init(project=project_name, name=run_name, config=locals())\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'fc' not in name:  # Assuming final layer is named 'fc'\n",
    "                param.requires_grad = False\n",
    "        print(\"‚úì Backbone frozen for initial training.\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # Scheduler selection\n",
    "    if lr_scheduler == 'onecycle':\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=config['lr'], epochs=config['epochs'], steps_per_epoch=len(train_loader)\n",
    "        )\n",
    "        step_per_batch = True\n",
    "    elif lr_scheduler == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        step_per_batch = False\n",
    "    elif lr_scheduler == 'step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        step_per_batch = False\n",
    "    else:\n",
    "        scheduler = None\n",
    "        step_per_batch = False\n",
    "\n",
    "    # Initialize checkpoint directory\n",
    "    checkpoint_path = Path(checkpoint_dir)\n",
    "    checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    start_epoch = 1\n",
    "    best_acc = 0.0\n",
    "    latest_checkpoint = checkpoint_path / \"latest_checkpoint.pth\"\n",
    "\n",
    "    if resume_from_checkpoint and latest_checkpoint.exists():\n",
    "        start_epoch, best_acc = load_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            checkpoint_path=str(latest_checkpoint),\n",
    "            scheduler=scheduler,\n",
    "            device=device\n",
    "        )\n",
    "        start_epoch += 1  # Start from next epoch\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        # Unfreeze backbone if specified\n",
    "        if unfreeze_epoch > 0 and epoch == unfreeze_epoch:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=weight_decay)\n",
    "            print(f\"Backbone unfrozen at epoch {epoch} with reduced Learning Rate.\")\n",
    "            wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device, True, scheduler, step_per_batch)\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, None, device, False)\n",
    "\n",
    "        wandb.log({\n",
    "            'train/loss': train_loss, 'train/acc': train_acc,\n",
    "            'val/loss': val_loss, 'val/acc': val_acc,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "        # Step scheduler per epoch if not per batch\n",
    "        if scheduler and not step_per_batch:\n",
    "            scheduler.step()\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Best model saved: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save checkpoint every checkpoint_interval epochs\n",
    "        if epoch % checkpoint_interval == 0:\n",
    "            save_checkpoint(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch,\n",
    "                target_dir=checkpoint_dir,\n",
    "                checkpoint_name=f\"checkpoint_epoch_{epoch}.pth\",\n",
    "                scheduler=scheduler,\n",
    "                best_acc=best_acc\n",
    "            )\n",
    "            # Also save as latest checkpoint for easy resuming\n",
    "            save_checkpoint(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch,\n",
    "                target_dir=checkpoint_dir,\n",
    "                checkpoint_name=\"latest_checkpoint.pth\",\n",
    "                scheduler=scheduler,\n",
    "                best_acc=best_acc\n",
    "            )\n",
    "\n",
    "    wandb.finish()\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a847acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full(config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Get data\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        dataset_name=config['dataset'],\n",
    "        batch_size=config['batch_size'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory'],\n",
    "        use_pretrained=config['use_pretrained'],\n",
    "        pretrained_model_name=config['pretrained_model_name']\n",
    "    )\n",
    "\n",
    "    # Get model\n",
    "    model = get_model(config['model'], config['num_classes'], config['dropout'], config['use_pretrained'])\n",
    "        # üîç DEBUG: Check model output shape and labels\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DEBUG INFO\")\n",
    "    print(\"=\"*60)\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sample_inputs, sample_labels = sample_batch\n",
    "    print(f\"Sample input shape: {sample_inputs.shape}\")\n",
    "    print(f\"Sample labels shape: {sample_labels.shape}\")\n",
    "    print(f\"Label range: [{sample_labels.min()}, {sample_labels.max()}]\")\n",
    "    print(f\"Config num_classes: {config['num_classes']}\")\n",
    "\n",
    "    # Test forward pass on CPU first\n",
    "    model = get_model(config['model'], config['num_classes'], config['dropout'], config['use_pretrained'])\n",
    "    test_output = model_cpu(sample_inputs[:2])  # Just 2 samples\n",
    "    print(f\"Model output shape: {test_output.shape}\")\n",
    "    print(f\"Expected: (2, {config['num_classes']})\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Train\n",
    "    best_acc = train_engine(\n",
    "        model, train_loader, val_loader, device,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['lr'],\n",
    "        lr_scheduler=config['lr_scheduler'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        project_name=config['project'],\n",
    "        run_name=config['run_name'],\n",
    "\n",
    "        freeze_backbone=config['freeze_backbone'],\n",
    "        unfreeze_epoch=config['freeze_epochs'],\n",
    "\n",
    "        checkpoint_dir=config['checkpoint_dir'],\n",
    "        resume_from_checkpoint=config['resume_from_checkpoint'],\n",
    "        checkpoint_interval=config['checkpoint_interval']\n",
    "    )\n",
    "\n",
    "    print(f\"Training complete! Best accuracy: {best_acc:.2f}%\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "============================================================\n",
      "[Transfer Learning Mode]\n",
      "Model: vit_base_patch16_224\n",
      "Input size: 224x224\n",
      "Statistics: ImageNet (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "Augmentation: Strong (auto_augment, flip, erase)\n",
      "============================================================\n",
      "\n",
      "Dataset: CIFAR100\n",
      "Mean/Std: (0.485, 0.456, 0.406) / (0.229, 0.224, 0.225)\n",
      "Train: 45000, Val: 5000, Test: 10000\n",
      "Batch size: 256, Num workers: 4, Pin memory: True\n",
      "\n",
      "\n",
      "============================================================\n",
      "DEBUG INFO\n",
      "============================================================\n",
      "Sample input shape: torch.Size([256, 3, 224, 224])\n",
      "Sample labels shape: torch.Size([256])\n",
      "Label range: [0, 99]\n",
      "Config num_classes: 100\n"
     ]
    }
   ],
   "source": [
    "model = train_full(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148933a",
   "metadata": {},
   "source": [
    "## 05. Evaluate w Test Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b103342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model evaluation with essential metrics\n",
    "\"\"\"\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, test_loader, device, class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate model and return comprehensive metrics.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model to evaluate\n",
    "        test_loader: DataLoader for test set (use ONLY for final evaluation!)\n",
    "        device: torch.device for evaluation\n",
    "        class_names: List of class names for classification report\n",
    "\n",
    "    Returns:\n",
    "        dict: {'accuracy',           # Overall accuracy\n",
    "               'f1_macro',           # f1 score for macro average\n",
    "               'f1_weighted',        # f1 score for weighted average\n",
    "               'confusion_matrix',   # Confusion matrix\n",
    "               'per_class_acc',      # Per-class accuracy\n",
    "               'classification_report',\n",
    "               'predictions',        # Predicted labels\n",
    "               'targets',            # True labels\n",
    "\n",
    "    Note:\n",
    "        - Use this function ONLY for final test set evaluation\n",
    "        - Load the best model checkpoint before calling this\n",
    "        - Do NOT use this during training/validation - it should be called once at the end\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = 100. * (all_preds == all_targets).sum() / len(all_targets)\n",
    "    f1_macro = f1_score(all_targets, all_preds, average='macro')\n",
    "    f1_weighted = f1_score(all_targets, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    # Per-class accuracy\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        all_targets, all_preds,\n",
    "        target_names=class_names,\n",
    "        digits=3\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.3f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.3f}\")\n",
    "    print(f\"\\n{report}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class_acc': per_class_acc,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ce9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_loader = get_dataloaders(\n",
    "        dataset_name=config['dataset'],\n",
    "        batch_size=config['batch_size'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory'],\n",
    "        use_pretrained=config['use_pretrained'],\n",
    "        pretrained_model_name=config['pretrained_model_name']\n",
    "    )\n",
    "\n",
    "eval_dict = evaluate(model, test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab72b68",
   "metadata": {},
   "source": [
    "## 06. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013968aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization utilities using wandb\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "def _require_run():\n",
    "    if wandb.run is None:\n",
    "        raise RuntimeError(\"No active W&B run. Call wandb.init(...) before logging.\")\n",
    "\n",
    "def log_confusion_matrix_from_matrix(cm, class_names, title=\"Confusion Matrix\"):\n",
    "    _require_run()\n",
    "    df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    wandb.log({\n",
    "        \"confusion_matrix\": wandb.plot.heatmap(\n",
    "            df,\n",
    "            x_label=\"Predicted\",\n",
    "            y_label=\"True\",\n",
    "            title=title\n",
    "        )\n",
    "    })\n",
    "\n",
    "def log_per_class_metrics(per_class_acc, class_names):\n",
    "    \"\"\"\n",
    "    Log per-class accuracy as wandb bar chart\n",
    "\n",
    "    Args:\n",
    "        per_class_acc: array of per-class accuracies\n",
    "        class_names: list of class names\n",
    "    \"\"\"\n",
    "    _require_run()\n",
    "    # Create bar chart data\n",
    "    data = [[class_names[i], per_class_acc[i] * 100] for i in range(len(class_names))]\n",
    "    table = wandb.Table(data=data, columns=[\"Class\", \"Accuracy (%)\"])\n",
    "\n",
    "    wandb.log({\n",
    "        \"per_class_accuracy\": wandb.plot.bar(\n",
    "            table, \"Class\", \"Accuracy (%)\",\n",
    "            title=\"Per-Class Accuracy\"\n",
    "        )\n",
    "    })\n",
    "    print(\"‚úì Per-class accuracy logged to wandb\")\n",
    "\n",
    "def log_sample_predictions(model, test_loader, device, class_names, num_samples=16):\n",
    "    \"\"\"\n",
    "    Log sample predictions to wandb\n",
    "\n",
    "    Args:\n",
    "        model: trained model\n",
    "        test_loader: test dataloader\n",
    "        device: torch device\n",
    "        class_names: list of class names\n",
    "        num_samples: number of samples to log\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get one batch\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images[:num_samples].to(device), labels[:num_samples]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "    # Create wandb images with predictions\n",
    "    _require_run()\n",
    "    wandb_images = []\n",
    "    for idx in range(num_samples):\n",
    "        img = images[idx].cpu()\n",
    "        true_label = class_names[labels[idx]]\n",
    "        pred_label = class_names[predicted[idx].cpu()]\n",
    "        pred_prob = probs[idx][predicted[idx]].cpu().item()\n",
    "\n",
    "        caption = f\"True: {true_label} | Pred: {pred_label} ({pred_prob:.2%})\"\n",
    "        wandb_images.append(wandb.Image(img, caption=caption))\n",
    "\n",
    "    wandb.log({\"predictions\": wandb_images})\n",
    "    print(f\"‚úì {num_samples} sample predictions logged to wandb\")\n",
    "\n",
    "def log_all_metrics(results, class_names):\n",
    "    \"\"\"\n",
    "    Log all evaluation metrics to wandb\n",
    "\n",
    "    Args:\n",
    "        results: dict from evaluate() function\n",
    "        class_names: list of class names\n",
    "    \"\"\"\n",
    "    _require_run()\n",
    "    # Log summary metrics\n",
    "    wandb.log({\n",
    "        \"test/accuracy\": results['accuracy'],\n",
    "        \"test/f1_macro\": results['f1_macro'],\n",
    "        \"test/f1_weighted\": results['f1_weighted'],\n",
    "    })\n",
    "\n",
    "    # Log per-class accuracies\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        wandb.log({f\"test/accuracy_{class_name}\": results['per_class_acc'][i] * 100})\n",
    "\n",
    "    print(\"‚úì All metrics logged to wandb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
